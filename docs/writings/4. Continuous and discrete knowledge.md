
# Continuous and discrete knowledge

Surely, there are many criteria to classify knowledge. The important thing is how much insight we can get from the classification. In this article, I will introduce how to use the idea of continuous and discrete to classify knowledge.

## Discrete knowledge

### What is discrete knowledge?

::: tip Definition
Discrete knowledge is the ones whose state is defined in a discrete space. Variation on it cannot be infinitesimal.
:::

For example, a coin has two states: head and tail. The state of a coin is discrete knowledge.

More importantly, logic deductions are operating discrete knowledge. All the system with a flavour of **logic** and have a clear border of what is true and what is wrong, e.g., knowledge graph and symbolic deductions, are mainly operating discrete knowledge.  
### What is the property of discrete knowledge?

Discrete knowledge is clear and easy to operate with computers. They can ensure 100% correctness given correct assumptions. For fields that have a concrete assumption, e.g., mathematics, discrete knowledge and its deduction will suffice.

However, not all fields have concrete assumptions. In the long debate of rationalism and empiricism, people found that it is absolutely not easy to find reliable and non-trivial assumption to reason from (See Kant and Hume). 

## Continuous knowledge

### What is continuous knowledge?

::: tip Definition
Continuous knowledge is the ones whose state is defined in a continuous space. It allows an infinitesimal variation.
:::

For example, the probability that a coin will be head is continuous knowledge. The probability is a real number between 0 and 1.

More importantly, neural networks hold continuous knowledge. The state of a neural network is defined by the weights of the connections between neurons. The weights are real numbers, which is a continuous space.


### How to tell whether the knowledge is continuous?

It might be tricky to check whether a piece of knowledge is continuous or not. The key is to imagine whether the knowledge can have a very small variation and still remain mostly true. For example, when you try to recall a voice of someone, you can never ensure that your memory today is the same as your memory yesterday. It also works for smell, visual or kinetic memory.  

Most importantly, though also containing discrete knowledge like grammar, a large part of our **knowledge about language** is also continuous. For example, your **feeling** about a certain word is continuous. The most obvious example is brands. You must have a certain feeling about Coca-cola, Pepsi, Tesla and BMW; and they don't have a clear border of correctness, nor you can check your feeling is stable.

### What is the property of continuous knowledge?

The representation power of continuous knowledge is much stronger than discrete knowledge. It is very hard to imagine how to represent the feeling of ski or recalling a picture with a discrete format. 

Continuous knowledge is more natural for human to process. Most of the physics theory also assume that the space is continuous or its discreteness is negligible for human. The power of continuous knowledge can also be proved by the success of neural network. There was a shift of the paradigm of *artificial intelligence* in the 1990s from discrete to continuous and then follows the triumph of neural networks in nearly all the field. 

However, the intrinsic drawbacks of continuous knowledge are still there. Even in 2023, we still cannot handle math, logic and coding satisfactorily with neural networks. This is surely because of the discrete nature of these tasks. How to bridge continuous knowledge with discrete knowledge will be the main challenge of building AI.


## How all this related to EvoNote?

EvoNote is trying to add more discrete structure to the continuous knowledge. 

Here, we first claim that the knowledge need to be interpreted by large language models are continuous. Though they might look like discrete because they are symbols, but they are meaningless symbols without an interpreter. 

> Admittedly, you can parse a sentence into a syntax tree. But syntax tree can never represent the accurate meaning. For example, I can set a question: 
> "If apple means eat in the next sentence. 'Mike apple an apple.' What did Mike intake?"
>This question is easy for human to answer but will break any natural language parser.

The way we want to do this, is to use the tree structure to organize the natural languages in a macro scale (Recall the section: Tree indexing). This can assign the continuous knowledge a discrete structure (tree), which we believe can help building a continuous-discrete hybrid knowledge to help making AI capable at discrete tasks.

