
# Indexing is understanding

What does it mean by understanding? Usually, it is use as the opposite of just memorizing. 
When you memorize something, you just remember it. But when you understand something, there are a few more magic happening:

1. You know which part of the knowledge is relevant to the context.

This is quite difficult because the context usually doesn't match the knowledge exactly. For example, if the doctor says: "don't drink any water". You may think that you can drink juice, but you can't drink juice either. You can't drink anything.

2. You know how different parts of the knowledge are related.

This is even more difficult. The reason is two-fold:

- Nearly every knowledge can be related in some way. For example, PNAS sounds like peanuts. Gravity and computer are related to apple by Newton and Turing. All these relations are true, but they are merely dry humor and not useful.
- Important relations are usually not obvious. For example, the observation of the movement of celestial bodies proves the existence of gravity. But no one notices this relation until Newton.

# Ancient approaches

The most important example of the use of indexing is search engine. Search engines collect the keywords in the documents and index them. When you search for a keyword, the search engine will return the documents that contain the keyword. This is the most basic form of indexing.

Search engines provide an efficient way to find a webpage with some keywords. However, you cannot imagine you discover the gravity by searching "apple" in Google. This is because the search engine doesn't understand the relation between the keywords. It only knows that the keywords appear in the same document.

A way to analyze the relation and the context of the keywords is to use a knowledge graph. A knowledge graph is a graph that contains the objects and the relations between them. For example, `juice` and `water` can be two nodes linked by an edge `has ingrediant`. In this way, it might help understanding whether you can drink juice when the doctor says "don't drink any water".

However, obviously, none of these ancient approaches has a chance to draw a relation between celestial bodies and gravity. Ever after decades of development, they still struggle with understanding everything that is a little bit abstract. Certainly we need something new.

# In the large language model (LLM) era

I will not introduce the history of the reason why LLMs work. But I believe every one of you who are reading this article must have some feeling that LLM can understand abstract mathematical concepts. If you ask ChatGPT:
```
When I dress myself, I can put on my shirt and then my pants. 
I can also put on my pants and then my shirt. It won't make a difference.

What mathematical concept is this?
```
ChatGPT will recognize that the order of the two actions doesn't matter, so it is related to commutativity. The answer won't change a lot if you modify the situation as long as it represents the concept of commutativity. It's quite hard to imagine how a search engine or a knowledge graph can do this. The word "commutativity" doesn't even appear in the question.

However, this good performance of LLM is at a cost. The most important limitation is that LLM the size of its input is limited, and it is completely not at the same scale as the knowledge graph and traditional search engines. You have to decide what is the most important context that the LLM have to know before you ask the question. This, again, requires some understanding of the knowledge. **LLM helps you the best when you already have some understanding of the knowledge.**

## Embedding-based indexing

Good news is that LLM not only helps us by directly giving the answer. It also helps us index existing knowledge. Notice that LLM are built with deep learning technology, in which neural networks are used to process the knowledge. In the intermediate layers of the neural network, the knowledge is represented as vectors called **embeddings**. 

These embeddings carry all the information about the input and have already been processed by the neural network for abstract understanding. Therefore, if two inputs to the LLM have similar embeddings, they are likely to be related, even in an abstract way. This is the key idea of **embedding-based indexing**.

Giving a few pieces of knowledge, we can use LLM to generate their embeddings as their index. Whenever a context is given, we generate the embedding of the context and find the similar embeddings in the knowledge base. This will have the model to gain essential knowledge before answering a question. Importantly, this embedding similarity-based indexing is totally scalable, meaning that you have the chance to index the knowledge of astronomy and gravity together!

# Wrap up

Though there might be still a lot of steps before we let the model rediscover gravity, we have already seen the potential of LLM in indexing. Importantly, we find a good roadmap to solve the two problems we put in the beginning. For the first, by embedding similarity, we have the tool for finding the relevant knowledge to the context and retrieve them. "Don't drink water" will have a high similarity with "don't drink juice". For the second, with the abstract understanding ability of LLM, we can extract relation of two pieces of knowledge. It can discover that "don't drink water" actually means "don't drink any liquid".

With embedding-based search in hand, it seems what left for us to build is simply improve its performance. However, you will find it as a surprisingly tricky task which involves much philosophical effort. Let's discuss it in another article.


# Related works

History of retrieval: https://dl.acm.org/doi/pdf/10.1145/3486250

LlamaIndex: https://www.llamaindex.ai/

ACL 2023 Tutorial on Retrieval-based Language Models: https://acl2023-retrieval-lm.github.io/